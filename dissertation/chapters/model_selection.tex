%begin-include
{\color{blue} Simple description of the content of this chapter}.

\section{Model ranking using Marginal Likelihood}
% Simple explanation of this model ranking
% Describe the likelihood function 
% However, this is very hard to calculate
% It is possible to apply an Importance Sampling Estimator {cite 
% Newton and Raftery}. However, it was showed on {cite girolami again}
% that these methods do not perform well.
% Then it was proposed to use Thermodynamic Integration. Make it clear 
% that this allows us to create some estimators.
% Subsection - Thermodynamic Integration
% -> explain how to derive it
%   -> name what is the power posterior
% -> include here: how to estimate it?
% Subsection - Estimation of the Marginal Likelihood 
% -> We need to find samples of the posteriors
% -> Explain that we use the methodology of Xuan
% -> We use three Metropolis-Hastings
% -> A first burn-in step with jumps independent for each single 
% parameter. Adaptive Metropolis Hastings
% -> A second using a Variance-Covariance Matrix
% -> A third using populational MCMC
The marginal likelihood of an experiment measurement $D$ being 
reproduced by a model $M$, $p (D | M)$, can be used as a model ranking 
metric as it determines which model makes the experimental observations 
more likely to happen. Before defining how to calculate the marginal 
likelihood, we must define what the likelihood function is. To calculate 
the likelihood $p (D | M, \theta)$, we must understand that conditioning 
the observation to the model and parameters means that in 
the probability space from which $D$ is taken, the model $M$ is the 
``real'' model and it has the parameter values of $\theta$; i.e. the 
model $M$ with parameters $\theta$ controls the behaviour of the system 
from which $D$ was observed. Then, assuming that the observations have a 
Gaussian error, and that they are taken in a time series of $m$ time 
steps, we can define the likelihood as:
\begin{equation}
    p (D | M, \theta) = p_{\mathcal{N}_{\left(\vec{0}, \Sigma\right)}}
        (\phi (M,\theta) - D),
\label{eq:likelihood_multivariate}
\end{equation}
where $\phi (M, \theta) \in \fieldR^m$ is the experimental measurement 
on the simulation generated by the model $M$ with parameters $\theta$,
and \smash{$p_{\mathcal{N}_{(\vec{0}, \Sigma)}} (\cdot)$} is the 
probability density function of a Multivariate Normal variable with mean 
$\vec{0}$ and covariance matrix $\Sigma$. As a matter of fact, as it is
done in the work of Xu et al. (2010), we can consider that the 
observation error is independent for each time step~\cite{Xura20}, 
therefore we can simplify~\ref{eq:likelihood_multivariate} to:
\begin{equation}
    p (D | M, \theta) = \prod_{i = 1}^m p_{\mathcal{N}_{\left(0, 
        \sigma^2\right)}} (\phi_i (M,\theta) - D_i).
\label{eq:likelihood}
\end{equation}
The $\sigma^2$ used in equation~\ref{eq:likelihood} is also a parameter
of the model, which means that, for some $k$, $\theta_k = \sigma^2$.

Now that we defined the likelihood function, we can write the marginal 
likelihood as:
\begin{equation}
    p (D | M) = \int_{\Theta} p (D | M, \theta) p (\theta | M)d\theta.
\label{eq:marginal_likelihood}
\end{equation}
However, calculating this integral analytically is only possible in 
very special cases and, usually, it would depend on knowing models for 
the distributions associated to these probability functions, which is 
generally not possible in our case.

Even though this integral is very hard to be calculated, there are 
methods that allow us to estimate its value. A straight forward method 
to estimate this integral value is using Importance Sampling 
Estimators~\cite{Newton1993}. This method uses the Monte Carlo integral 
estimation method that can estimate integrals of the form 
$\int g(\lambda) p(\lambda)d\lambda$ using the estimator:
\begin{equation*}
    \hat{I} = \sum_{i = 1}^m w_i g(\lambda_i) / \sum_{i = 1}^m w_i,
\label{eq:importance_sampling_estimator}
\end{equation*}
where $w_i = p (\lambda) / p^* (\lambda)$, and $p^*(\cdot)$ is known as 
the importance sampling function. If we set $\lambda = \theta | M$ and
use the prior ($p(\theta | M)$) or the posterior ($p(\theta | M, D)$) as 
importance sampling functions, then we would get respectively the 
estimators:
\begin{equation*}
\begin{aligned}
    \frac{1}{m} \sum_{i = 1}^m p(D|M, \theta^{(i)}) &&& 
        (\text{with } \theta^{(i)} \sim p(\theta|M)), \\
    \left(\frac{1}{m} \sum_{i = 1}^m p(D|M, \theta^{(i)})^{-1} \right)^{-1} &&&
        (\text{with } \theta^{(i)} \sim p(\theta|M, D)).
\end{aligned}
\end{equation*}
However, as showed by Vyshemirsky et al. (2007), these estimators might
produce very large variances and may not perform well for biochemical
model selection applications. Hence, new methods with ideas of 
thermodynamics were proposed. These methods are based on rewriting the
marginal likelihood equation using intermediate distributions of 
parameters between the prior and posterior 
distributions~\cite{Friel2008}.

% Subsection - Thermodynamic Integration
% -> explain how to derive it
%   -> name what is the power posterior
% -> include here: how to estimate it?
\subsection{Thermodynamic Integration for Marginal Likelihood}
The Thermodynamic Integration is a method that proposes to rewrite
the integral~\ref{eq:marginal_likelihood} using ideas of thermodynamics,
providing new estimators for the marginal likelihood. This method is
able to rewrite the marginal likelihood integral in a way that it 
marginalizes the likelihood through many intermediate probability spaces 
of parameters, bridging the prior and posterior distributions of 
parameters. These distributions are also called tempered distributions
or power posteriors~\cite{Friel2008}.

Given a parameter prior distribution $p (\theta | M)$ and a posterior 
distribution $p (\theta | D, M)$, then we define a power posterior 
distribution with temperature $\beta$ as:
\begin{equation*}
    p_{\beta} (\theta) = \frac{p (D | \theta, M)^\beta p(\theta | M)}
                              {z (\beta)},
\end{equation*}
where
\begin{equation*}
    z (\beta) = \int_\Theta p (D | \theta, M)^\beta p(\theta | M) 
            d\theta.
\end{equation*}
Note that when $\beta = 0$, then $p_{\beta=0} = p (\theta | M)$, the 
prior distribution of the parameters; also, when $\beta = 1$, then
\begin{equation*}
    p_{\beta=1}(\theta) = \frac{p (D | \theta, M) p(\theta | M)}
                         {z (\beta)}
                        =\frac{p (D, \theta|M)}
                              {\int_\Theta p (D, \theta | M)d\theta}
                        =\frac{p(\theta | D, M) p(D|M)}{p (D | M)}
                        =p (\theta | D, M),
\end{equation*}
the posterior distributions of the parameters.

Now, let we consider the derivative of $\ln z(\beta)$.
\begin{align}
    \frac{d}{d\beta} \ln z(\beta) &= \frac{1}{z(\beta)}  
        \frac{d}{d\beta} z(\beta) \notag\\
    &= \frac{1}{z(\beta)} \frac{d}{d\beta} 
        \int_\Theta p (D | \theta, M)^\beta p(\theta | M) d\theta \notag\\
    \shortintertext{(using that $\frac{d}{dx} c^x = c^x \ln c$)}
    &= \frac{1}{z(\beta)} \int_\Theta p (D | \theta, M)^\beta 
        p(\theta | M) \ln p(D|\theta, M)d\theta \notag\\
    &= \int_\Theta \frac{p (D | \theta, M)^\beta p(\theta | M)}
                        {z(\beta)}
                    \ln p(D| \theta, M)d\theta \notag\\
    &=\int_\Theta p_\beta (\theta) \ln p (D | \theta, M)d\theta \notag\\ 
    &=\expectation_{p_\beta (\theta)} [\ln p(D|\theta, M)]. \label{eq:z_derivative}
\end{align}
And it is not hard to see that:
\begin{equation}
\begin{gathered}
    z (0) = \int_\Theta p (\theta | M)d\theta = 1 \\
    z (1) = \int_\Theta p (D, \theta | M)d\theta = p (D|M) 
    \label{eq:z_on_limits}
\end{gathered}
\end{equation}

Using equations~\ref{eq:z_on_limits} and equality~\ref{eq:z_derivative}
we can write that:
\begin{align}
    \int_0^1 \expectation_{p_\beta (\theta)} [\ln p(D|\theta, M)]d\beta 
    &= \int_0^1 \frac{d}{d\beta} \ln z(\beta) d\beta \notag \\
    &= \Big[\ln z(\beta)\Big]\bigg\rvert^1_0 \notag \\
    &= \ln p (D | M).
    \label{eq:thermodynamic_integral}
\end{align}
Then we have written an expression for the logarithm of the marginal
likelihood. This expression is still hard to be calculated analytically,
however from this equation we will be able to find estimators for
the logarithm of the marginal likelihood, and consequently for the
likelihood. To calculate this estimators, we will need to generate 
samples for a series of power posteriors of parameters, and this will
be explained in the next section.

% Subsection - Estimation of the Marginal Likelihood 
% -> We need to find samples of the posteriors
% -> Explain that we use the methodology of Xuan
% -> We use three Metropolis-Hastings
% -> A first burn-in step with jumps independent for each single 
% parameter. Adaptive Metropolis Hastings
% -> A second using a Variance-Covariance Matrix
% -> A third using populational MCMC
\subsection{Estimation of the Marginal Likelihood}
There are multiple approaches on estimating the 
integral~\ref{eq:thermodynamic_integral}, and for all of them, it is 
necessary to find samples of $p_{\beta_t} (\theta | M, D)$ for a 
sequence of values of $\beta_t$ that vary from zero to one~\cite{Xura20, 
Vyshemirsky2007, Friel2008}. The differences on these methods are mainly 
on the choice of the sequence $\{\beta_1, \beta_2, \ldots, \beta_T\}$,
on the Metropolis-Hastings (MH) algorithms used, and finally on the 
estimator.

We are now going to present the methodology we used to estimate 
parameter values. First, we assume that there is a chosen sequence of
$\{\beta_1, \beta_2, \ldots, \beta_T\}$ to explain how to generate 
samples of power posterior distributions with these temperatures. Then,
we explain how to choose this values and present the estimator we used
to estimate $p (D | M)$.

\subsubsection{Power posteriors sampling}
Given a sequence of temperatures, the method used to sample from the
power posteriors has three different steps, all of them using 
Metropolis-Hastings algorithms, similarly to what is done by Xu et al. 
(2010). For each of the $T$ temperatures, the first two phases are
run separately, generating $T$ chains that are samples of the power 
posteriors. Then, on the third phase, each chain continues to grow, but
not separately because two random chains will have their last accepted 
point swapped, what causes the chains to be mixed; this approach on 
mixing these chains is called Populational Monte Carlo Markov 
Chain (Populational MCMC)~\cite{Friel2008}.

The first step of sampling is run independently for each temperature. 
The Metropolis-Hastings performed on this step is started taking a 
sample from the parameter priors, and the proposal distribution is 
a multivariate log-normal that has mean on the last point sampled and
with a diagonal covariance matrix, i.e. parameters are proposed 
independently. This covariance matrix is updated repeatedly after a 
predefined number of iterations with the goal of adapting the proposal 
distribution to the parameter space. This update is performed as 
proposed in Gelman et al. (2013): 
\begin{itemize}
\item{if the acceptance rate of parameter points in the last 
    iterations is greater than $0.44$, then increase the variance of the
    jump;}
\item{if the acceptance rate is lower than $0.23$, then decrease the 
    variance of the jump.}
\end{itemize}
Recalling that our target function (sampling function) is 
$p (\theta | M, D)$, then we have that the probability of accepting a 
proposed parameter $\theta^*$, given that the current point is 
$\theta^{(t, i)}$, is $\min (1, r)$ with
\begin{equation*}
    r = \frac{p (\theta^* | M, D)}
             {p (D | \theta^{(t, i)})}
        \frac{J_{(t, i)} (\theta^{(t, i)} | \theta^*)}
             {J_{(t, i)} (\theta^* | \theta^{(t, i)})}
\end{equation*}
with $J$ being the 


%\subsection{Implementation of SigNetMS}
% -> Python
% -> Uses the libSBML python package





% ABC-SysBio
% What do I want to talk about ABC-SysBio?
% It is a likelihood-free approach for generating samples of a posterior
% distribution.
% Simply define the method: what does it do exactly?
% -> it approximates some posterior distribution without using a 
%    likelihood
% Give the generic steps of the algorithm
% Give as an example the ABC-Rejection algorithm
% Explain the result
% How would we get model ranking from this result?
% The ABC-SysBio Software
\section{Approximate Bayesian Computation}
Approximate Bayesian Computation (ABC) is an approach that allows 
generating samples from the posterior distribution without accessing the
likelihood function. Adding a model indicator parameter to parameters 
being sampled also allows us to create an approximate sample of the 
posterior $p (\theta, M | D)$, and from this sample it is possible to 
estimate $p (M | D)$, which can be used as a model ranking metric. The 
main idea of ABC methods is to generate a sample from posterior by 
generating parameter points that, when plugged into a model, generates 
simulations that dist from the experimental measurements at most by some
small $\epsilon$.

A generic ABC method that generates a sample of the posterior 
$p (\theta, m | D)$ is composed of the following steps:
\begin{enumerate}
    \item{Sample a parameter candidate $(\theta^*, M^*)$ from some 
        proposal distribution.} \label{alg:abc_step1}
    \item{Simulate the model $M^*$ with parameter values $\theta^*$, 
        generating simulated measurements $\phi (M^*, \theta^*) = D*$.}
    \item{Calculate, for some distance function $d$, the value of
        $d (D^*, D)$. If $d (D^*, D) < \epsilon$ for some previously
        specified $\epsilon$, then add $(\theta^*, M^*)$ to the sample.}
    \item{Repeat until some iteration limit.}
\end{enumerate}

The simplest ABC algorithm is the ABC Rejection, and it goes very 
similarly to the generic algorithm we just presented, with the 
specification that on step~\ref{alg:abc_step1} the proposal distribution
is the prior distribution. The output of this algorithm is a sample of 
the distribution $p (\theta, M| d (\phi (M, \theta), D) \leq \epsilon)$; 
when $\epsilon \to \infty$ this is then a sample of the prior 
distribution, and as $\epsilon \to 0$ then this sample tends to be a
sample of the posterior distribution~\cite{Pritchard1999}. This 
algorithm, however, does not perform well when the posterior 
distribution is very different from the prior distribution. For that 
reason, new ABC methods using Monte Carlo Markov Chains were 
created~\cite{Marjoram2003}. The ABC MCMC method proposed by Marjoram et 
al. (2003) produces a Markov Chain whose stationary distribution is
$p (\theta, M | d (\phi (M, \theta), D) \leq \epsilon)$. Nonetheless, 
this algorithm might still suffer from correlation in samples or even
get stuck in regions of local peaks of probability. For that reason,
the ABC sequantial Monte Carlo (ABC SMC)method was 
proposed~\cite{Toni2009}.

%-> Explain ABC SMC
The ABC SMC method creates a sequence of samples with the goal of 
getting closer to a posterior sample in each step. Let we simplify the
notation by saying that $d(\phi (M, \theta), D)$ is just 
$\rho_{M, \theta}$. From a predefined sequence $\epsilon_1, \ldots, 
\epsilon_T$ the algorithm generates a sequence of samples that 
represents the distributions 
$p (\theta, M| \rho_{M, \theta} \leq \epsilon_1), 
p (\theta, M| \rho_{M, \theta} \leq \epsilon_2), \ldots,
p (\theta, M| \rho_{M, \theta} \leq \epsilon_T)$. At the first 
generation, a sample of 
$p (\theta, M| \rho_{M, \theta} \leq \epsilon_1)$ is created by 
proposing points from the prior distribution. Then, for next generations
the candidates to the sample are proposed based on the points of the 
last generation and their weight, plus some noise determined by a 
perturbation Kernel; the weight of a point $(\theta^*, M^*)$ on 
generation $t$ is a estimative of  
$p (\theta = \theta^*, M = M^* | \rho_{M, \theta} \leq \epsilon_t)$. The 
pseudo-code~\ref{code:abc_smc} presents the ABC SMC algorithm.

\begin{algorithm}[h]
\textsc{ABC SMC} $(\mathcal{M}, D)$
\begin{algorithmic}[1]
    \State Define the sequence $\epsilon_1, \ldots, \epsilon_T$.
    \State Define $N$, the sample size for each generation. 
    \State Sample $\{(\theta^{(1, 1)}, M^{(1, 1)}), 
                     (\theta^{(1, 2)}, M^{(1, 2)}), \ldots, 
                     (\theta^{(1, N)}, M^{(1, N)})\}$ from 
                     $p (\theta, M| D)$.
    \State Set $w^{(1, i)} = 1, \forall i \in {1, \ldots, N}$.  
    \For{$t \in \{1, \ldots, T\}$}
        \State $i \gets 1$
        \While{$i \leq N$}
            \State Sample $M^*$ from $p (M | D)$, the model prior.
            \State Sample $(\theta^{(t - 1, k)}, M^*)$ from the 
                last generation with weight $w^{(t - 1, k)}$.
            \State Create $(\theta^*, M^*)$ by perturbing 
                $\theta^{(t - 1, k)}$; 
                $\theta^* \propto K^t(\theta | \theta^{(t - 1, k)})$.
            \If{$p (\theta^* | M^*) = 0$}
                \State Continue to next iteration.
            \EndIf
            \State $D* \gets \phi (M^*, \theta^*)$
            \If{$d (D^*, D) \leq \epsilon$}
                \State $i \gets i + 1$
                \State $(\theta^{(t, i)}, M^{(t, i)}) \gets 
                    (\theta^*, M^*)$
            \EndIf
        \EndWhile
        \State Calculate the weights of the population:
            %\begin{equation*}
$                w^{(t, i)} = \frac{p (\theta^{(t, i)} | M^{(t, i)})}
                         {\sum_{j = 1}^N w^{(t-1, j)}p_{K^t}
                            (\theta^{(t-1, j)}, \theta^{(t, i)})}$
            %\end{equation*}
    \EndFor
    \Return
\end{algorithmic}
\caption{Pseudo-code of ABC SMC.}
\label{code:abc_smc}
\end{algorithm}
The ABC SMC algorithm is implemented in a software called 
ABC-SysBio~\cite{Liepe2014}. The software is implemented in Python and
the code is available on the SourceForge website. Similarly to the 
SigNetMS, ABC-SysBio takes as input SBML files with the description of
models, a prior definition for parameters, and experimental data.

% Experiments comparing both approaches
