%begin-include
{\color{blue} Simple description of the content of this chapter}.

\section{Model ranking using Marginal Likelihood}
% Simple explanation of this model ranking
% Describe the likelihood function 
% However, this is very hard to calculate
% It is possible to apply an Importance Sampling Estimator {cite 
% Newton and Raftery}. However, it was showed on {cite girolami again}
% that these methods do not perform well.
% Then it was proposed to use Thermodynamic Integration. Make it clear 
% that this allows us to create some estimators.
% Subsection - Thermodynamic Integration
% -> explain how to derive it
%   -> name what is the power posterior
% -> include here: how to estimate it?
% Subsection - Estimation of the Marginal Likelihood 
% -> We need to find samples of the posteriors
% -> Explain that we use the methodology of Xuan
% -> We use three Metropolis-Hastings
% -> A first burn-in step with jumps independent for each single 
% parameter. Adaptive Metropolis Hastings
% -> A second using a Variance-Covariance Matrix
% -> A third using populational MCMC
The marginal likelihood of an experiment measurement $D$ being 
reproduced by a model $M$, $p (D | M)$, can be used as a model ranking 
metric as it determines which model makes the experimental observations 
more likely to happen. Before defining how to calculate the  marginal 
the likelihood, we must define what the likelihood function is. To 
calculate the likelihood $p (D | M, \theta)$, we must understand that 
conditioning the observation to the model and parameters means that in 
the probability space from which $D$ is taken, the model $M$ is the 
``real'' model and it has the parameter values of $\theta$; i.e. the 
model $M$ with parameters $\theta$ controls the behaviour of the system 
from which $D$ was observed. Then, assuming that the observations have a 
Gaussian error, and that they are taken in a time series of $m$ time 
steps, we can define the likelihood as:
\begin{equation}
    p (D | M, \theta) = p_{\mathcal{N}_{\left(\vec{0}, \Sigma\right)}}
        (\phi (M,\theta) - D),
\label{eq:likelihood_multivariate}
\end{equation}
where $\phi (M, \theta) \in \fieldR^m$ is the experimental measurement 
on the simulation generated by the model $M$ with parameters $\theta$,
and \smash{$p_{\mathcal{N}_{(\vec{0}, \Sigma)}} (\cdot)$} is the 
probability density function of a Multivariate Normal variable with mean 
$\vec{0}$ and covariance matrix $\Sigma$. As a matter of fact, as it is
done in the work of Xu et al., we can consider that the observation 
error is independent for each time step~\cite{Xura20}, therefore we can 
simplify~\ref{eq:likelihood_multivariate} to:
\begin{equation}
    p (D | M, \theta) = \prod_{i = 1}^m p_{\mathcal{N}_{\left(0, 
        \sigma^2\right)}} (\phi_i (M,\theta) - D_i).
\label{eq:likelihood}
\end{equation}
The $\sigma^2$ used in equation~\ref{eq:likelihood} is also a parameter
of the model, which means that, for some $k$, $\theta_k = \sigma^2$.

Now that we defined the likelihood function, we can write the marginal 
likelihood as:
\begin{equation}
    p (D | M) = \int_{\Theta} p (D | M, \theta) p (\theta | M)d\theta.
\label{eq:marginal_likelihood}
\end{equation}
However, calculating this integral analytically is only possible in 
very special cases and, usually, it would depend on knowing models for 
the distributions associated to these probability functions, which is 
generally not possible in our case.

Even though this integral is very hard to be calculated, there are 
methods that allow us to estimate its value. A straight forward method 
to estimate this integral value is the Importance Sampling 
Estimator~\cite{Newton1993}. This method uses the Monte Carlo integral 
estimation method that can estimate integrals of the form 
$\int g(\lambda) p(\lambda)d\lambda$ using the estimator:
\begin{equation}
    \hat{I} = \sum_{i = 1}^m w_i g(\lambda_i) / \sum_{i = 1}^m w_i,
\label{eq:importance_sampling_estimator}
\end{equation}
where $w_i = p (\lambda) / p^* (\lambda)$, and $p^*(\cdot)$ is known as 
the importance sampling function. If we set $\lambda = \theta | M$ and
use the prior ($p(\theta | M)$) or the posterior ($p(\theta | M, D)$) as 
importance sampling functions, then we would get respectively the 
estimators:
\begin{equation}
\begin{aligned}
    \frac{1}{m} \sum_{i = 1}^m p(D|M, \theta^{(i)}) &&& 
        (\text{with } \theta^{(i)} \sim p(\theta|M)); \\
    \left(\frac{1}{m} \sum_{i = 1}^m p(D|M, \theta^{(i)})^{-1} \right)^{-1} &&&
        (\text{with } \theta^{(i)} \sim p(\theta|M)).
\end{aligned}
\end{equation}
However, as showed by Vyshemirsky et al. (2007), these estimators might
produce very large variances and may not perform well for biochemical
model selection applications. For that reason we decide to use a 
Annealing-Melting method as it is proposed in the same 
work~\cite{Vyshemirsky2007}.

% ABC-SysBio
% Experiments comparing both approaches
