%begin-include

In this chapter we will define the model selection problem in the 
context of this work and we will also present two state of the art 
methodologies that can be used to rank models, both of them are Bayesian
approaches. These methods create estimatives of $p ({\bm D} | M)$ or $p
(M | {\bm D})$ and use these values as model scores. 

The first approach is to estimate the marginal likelihood of the data 
${\bm D}$ being reproduced by a model $M$, $p ({\bm D} | M)$. To use
this method, it is necessary to define a likelihood function, $p({\bm D}
| M, {\bm \theta})$, and prior distributions of model parameters $p({\bm
\theta} | M)$. The estimation of this probability is done by taking
samples of tempered posteriors (to experimental data) of model
parameters. These tempered posterior distributions construct a bridge of
distributions connecting the prior and posterior parameter
distributions, allowing an estimation of the marginal likelihood. 

Another method is to use Approximate Bayesian Computation. ABC methods 
are used to estimate posterior (also to experimental data) distributions
of parameters. On this method, a sequence of populations of parameters
is created, and for each generation, the distance between experimental
data and data simulated by the population is decreased, leading to a
population that tends to be closer to a posterior distribution. In our
case, we add to ${\bm \theta}$ a model indicator parameter M, and then
we create a sample of $p({\bm \theta}, M | {\bm D})$. If we marginalize
this sample, we can calculate an estimative of the probability $p (M |
{\bm D})$. ABC methods allow simpler algorithms, and they also allow
selecting models without the neeed of a likelihood function, altough
defining the prior distribution of parameters is still needed.

\section{Elements of model selection}
% what is the model selection problem
%   -> the problem of finding the model that best represent experimental
%   data
%   -> these models contain parameters that are related to reaction
%   rates and observation errors.
%   -> We can list then three main entities of this problem: the models,
%   model parameters and experimental data.
% what do we consider as a model and how do we represent it
%   -> we consider a model as a set o chemical reactions with specified
%   initial concentrations.
%   -> a parameter prior distribution.
% what are model parameters and how do we represent it
% what is experimental data and how do we represent it
The model selection problem we consider in this work consists in the
selection of a biochemical model that can best simulate the dynamics 
of concentrations of chemical species, measured previously on a 
biological experiment. The models we consider are mathematically 
represented by a system of ordinary differential equations that
contains parameters related to reaction rates and experimental errors.
We can list three main entities of this problem: the set of models, the 
parameter space of each model, and the biological experimental data.
We shall then define how we represent these entities through this text.

We indicate a model with a random variable that assume natural values, 
$M: \Omega \to \naturals$. We consider that a model is not only bound to
a set of chemical reactions and its mathematical representation, but 
it is also bound to initial concentrations of chemical species and a
prior distribution of model parameters, which we represent as $p({\bm
\theta} | M)$.

Since we are using a Bayesian approach, we consider that model
parameters is a random vector ${\bm\theta} | M: \Omega \to \fieldR^{n}_+$,
where $d$ equals to the number of parameters in $M$ plus one, so we can
also represent one parameter related to experimental error. This
parameter represents the standard deviation of errors in the observation
of the experiment. It is important to note that all parameters are
positive, since reaction rates and standard deviations can only be
positive. That is an important information to choose a prior
distribution for model parameters.

The experimental data is a vector of values that describe the dynamics
of some measurement made on the biological phenomena of interest. We
represent data as ${\bm D} \in \fieldR^{d}$. As we described in
Section~\ref{sec:fund_concepts:measurements}, these measurements are
often created with Western Blot experiments, and the values of ${\bm D}$
usually represent the ratio of concentrations of proteins.

\section{Model ranking using Marginal Likelihood}
\label{sec:marginal_likelihood_method}
% Simple explanation of this model ranking
% Describe the likelihood function 
% However, this is very hard to calculate
% It is possible to apply an Importance Sampling Estimator {cite 
% Newton and Raftery}. However, it was showed on {cite girolami again}
% that these methods do not perform well.
% Then it was proposed to use Thermodynamic Integration. Make it clear 
% that this allows us to create some estimators.
% Subsection - Thermodynamic Integration
% -> explain how to derive it
%   -> name what is the power posterior
% -> include here: how to estimate it?
% Subsection - Estimation of the Marginal Likelihood 
% -> We need to find samples of the posteriors
% -> Explain that we use the methodology of Xuan
% -> We use three Metropolis-Hastings
% -> A first burn-in step with jumps independent for each single 
% parameter. Adaptive Metropolis Hastings
% -> A second using a Variance-Covariance Matrix
% -> A third using populational MCMC
The marginal likelihood of an experiment measurement ${\bm D}$ being 
reproduced by a model $M$, $p ({\bm D} | M)$, can be used as a model
ranking metric as it determines which model makes the experimental
observations more likely to happen. Before defining how to calculate the
marginal likelihood, we must define some likelihood function. To
understand our chosen likelihood function $p ({\bm D} | M, {\bm
\theta})$, we must understand that conditioning the observation to a
model and a set of parameters means that, in the probability space from
which ${\bm D}$ is taken, the model $M$ is the ``real'' model and it has
the parameter values of ${\bm \theta}$; i.e. the model $M$ with
parameters ${\bm \theta}$ controls the behaviour of the system from
which ${\bm D}$ was observed. Now, after understanding in which
probability space we are, we can assume that: 
% first: our phi function is deterministic
% second: the observations have a Gaussian error, and that they are 
% taken in a time series of $m$ time steps, we can define the likelihood
% as:
\begin{equation}
    p ({\bm D} | M,{\bm \theta}) = 
        p_{\mathcal{N}_{\left(\vec{0}, \Sigma\right)}}
        (\phi (M, {\bm\theta}) - {\bm D}),
\label{eq:likelihood_multivariate}
\end{equation}
where $\phi (M, {\bm \theta}) \in \fieldR^d$ is the experimental 
measurement on the simulation generated by the model $M$ with parameters
${\bm \theta}$, and \smash{$p_{\mathcal{N}_{(\vec{0}, \Sigma)}} 
(\cdot)$} is the probability density function of a Multivariate Normal 
random variable with mean $\vec{0}$ and covariance matrix $\Sigma$. Note
that we are considering that simulations of a model are deterministic,
as $\phi$ is function, and therefore the randomness of the model
arises from observation experimental error. We consider this random
error follows a Gaussian distribution, similarly to Xu and Vyshemirsky
\cite{Xura20}\cite{Vyshemirsky2007}.

As it is done in the work of Xu et al. (2010), we can consider that the 
observation error is independent for each time step~\cite{Xura20}, 
therefore we can simplify~\ref{eq:likelihood_multivariate} to:
\begin{equation}
    p ({\bm D} | M, {\bm \theta}) = \prod_{i = 1}^d p_{\mathcal{N}_{\left(0, 
        \sigma^2\right)}} (\phi_i (M, {\bm \theta}) - {\bm D}_i).
\label{eq:likelihood}
\end{equation}
The $\sigma^2$ used in Equation~\ref{eq:likelihood} is also a parameter
of the model, and as we explained on the last section, this parameter is
incorporated into ${\bm \theta}$, which means that, for some $k \in \{1,
..., n\}$, ${\bm \theta}_k = \sigma^2$.

Now that we defined the likelihood function, we can write the marginal 
likelihood. As the name suggests, this value comes from the
marginalization of the likelihood function, over the parameter space. We
can write as:
\begin{equation}
    p ({\bm D} | M) = \int_{\Theta} p ({\bm D} | M, {\bm \theta}) 
        p ({\bm \theta} | M)d{\bm\theta}.
\label{eq:marginal_likelihood}
\end{equation}
However, calculating this integral analytically is only possible in 
very special cases and, usually, it would depend on knowing models for 
the distributions associated to these probability functions, which is 
generally not possible in our case.

Even though this integral is very hard to be calculated, there are 
methods that allow us to estimate its value. A straight forward method 
to estimate this integral value is using Importance Sampling 
Estimators~\cite{Newton1993}. This method uses the Monte Carlo integral 
estimation method that can estimate integrals of the form 
$\int g(\lambda) p(\lambda)d\lambda$ using the estimator:
\begin{equation*}
    \hat{I} = \sum_{i = 1}^m w_i g(\lambda_i) / \sum_{i = 1}^m w_i,
\label{eq:importance_sampling_estimator}
\end{equation*}
where $w_i = p (\lambda) / p^* (\lambda)$, and $p^*(\cdot)$ is known as 
the importance sampling function. If we set $\lambda = {\bm\theta} | M$ 
and use the prior ($p({\bm \theta} | M)$) or the posterior ($p({\bm
\theta} | M, {\bm D})$) as importance sampling functions, then we would
get respectively the estimators:
\begin{equation*}
\begin{aligned}
    \frac{1}{m} \sum_{i = 1}^m p({\bm D}|M, {\bm \theta}^{(i)}) &&& 
        (\text{with } {\bm \theta}^{(i)} \sim p({\bm \theta}|M)), \\
    \left(\frac{1}{m} \sum_{i = 1}^m p({\bm D}|M, 
            {\bm \theta}^{(i)})^{-1} \right)^{-1} &&&
        (\text{with } {\bm \theta}^{(i)} \sim p({\bm \theta}|M, 
            {\bm D})).
\end{aligned}
\end{equation*}
However, as showed by Vyshemirsky et al. (2007), these estimators might
produce very large variances and may not perform well for biochemical
model selection applications. Hence, new methods with ideas of 
thermodynamics were proposed. These methods are based on rewriting the
marginal likelihood equation using intermediate distributions of 
parameters between the prior and posterior 
distributions~\cite{Friel2008}.

% Subsection - Thermodynamic Integration
% -> explain how to derive it
%   -> name what is the power posterior
% -> include here: how to estimate it?
\subsection{Thermodynamic Integration for Marginal Likelihood}
The Thermodynamic Integration is a method that proposes to rewrite
the Integral~\ref{eq:marginal_likelihood} using ideas of thermodynamics,
providing new estimators for the marginal likelihood. This method is
able to rewrite the marginal likelihood integral in a way that it 
marginalizes the likelihood through many intermediate probability spaces 
of parameters, bridging the prior and posterior distributions of 
parameters. These distributions are also called tempered distributions
or power posteriors~\cite{Friel2008}.

Given a parameter prior distribution $p ({\bm \theta} | M)$ and a 
posterior distribution $p ({\bm \theta} | {\bm D}, M)$, then we define a
power posterior distribution with ``temperature'' $\beta$ as:
\begin{equation*}
    p_{\beta} ({\bm \theta}) = 
        \frac{p ({\bm D} | {\bm \theta}, M)^\beta p({\bm \theta} | M)}
                              {z (\beta)},
\end{equation*}
where
\begin{equation*}
    z (\beta) = 
        \int_\Theta p ({\bm D} | {\bm \theta}, M)^\beta 
        p({\bm \theta} | M) d{\bm \theta}.
\end{equation*}
Note that when $\beta = 0$, then $p_{\beta=0} = p ({\bm \theta} | M)$, 
the prior distribution of the parameters; also, when $\beta = 1$, then
\begin{equation*}
    p_{\beta=1}({\bm \theta}) = 
        \frac{p ({\bm D} | {\bm \theta}, M) p({\bm \theta} | M)}
             {z (\beta)}
                        =
        \frac{p ({\bm D}, {\bm \theta} | M)}
             {\int_\Theta p ({\bm D}, {\bm \theta} | M)d{\bm \theta}}
                        =
        \frac{p({\bm \theta} | {\bm D}, M) p({\bm D} | M)}
             {p ({\bm D} | M)}
                        =
        p ({\bm \theta} | {\bm D}, M),
\end{equation*}
the posterior distributions of the parameters. Therefore, if we vary the
value of $\beta$ from $0$ to $1$ we can create a path of probability 
distributions that connect the prior and posterior distributions. We
will explain now, how this sequence of distributions are applied to
calculate the marginal likelihood.

Consider the derivative of $\ln z(\beta)$.
\begin{align}
    \frac{d}{d\beta} \ln z(\beta) &= \frac{1}{z(\beta)}  
        \frac{d}{d\beta} z(\beta) \notag\\
    &= 
    \frac{1}{z(\beta)} \frac{d}{d\beta} 
    \int_\Theta p ({\bm D} | {\bm \theta}, M)^\beta 
    p({\bm \theta} | M) d{\bm \theta} \notag\\
    \shortintertext{(using that $\frac{d}{dx} c^x = c^x \ln c$)}
    &= 
    \frac{1}{z(\beta)} \int_\Theta p ({\bm D} | {\bm \theta}, M)^\beta 
    p({\bm \theta} | M) \ln p({\bm D}|{\bm \theta}, M)
    d{\bm \theta} \notag\\
    &= 
    \int_\Theta \frac{p ({\bm D} | {\bm \theta}, M)^\beta 
        p({\bm \theta} | M)}
                     {z(\beta)}
    \ln p({\bm D}| {\bm \theta}, M)d{\bm \theta} \notag\\
    &=
    \int_\Theta p_\beta ({\bm \theta}) 
    \ln p ({\bm D} | {\bm \theta}, M)d{\bm \theta} \notag\\ 
    &=
    \expectation_{p_\beta ({\bm \theta})} 
    [\ln p({\bm D} | {\bm \theta}, M)]. \label{eq:z_derivative}
\end{align}
And it is not hard to see that:
\begin{equation}
\begin{gathered}
    z (0) = \int_\Theta p ({\bm \theta} | M)d{\bm \theta} = 1 \\
    z (1) = \int_\Theta p ({\bm D}, {\bm \theta} | M)d
        {\bm \theta} = p ({\bm D} | M)
    \label{eq:z_on_limits}
\end{gathered}
\end{equation}

Using equations~\ref{eq:z_on_limits} and Equality~\ref{eq:z_derivative}
we can write that:
\begin{align}
    \int_0^1 \expectation_{p_\beta ({\bm \theta})} 
        [\ln p({\bm D}|{\bm \theta}, M)]d\beta 
    &= \int_0^1 \frac{d}{d\beta} \ln z(\beta) d\beta \notag \\
    &= \Big[\ln z(\beta)\Big]\bigg\rvert^1_0 \notag \\
    &= \ln p ({\bm D} | M).
    \label{eq:thermodynamic_integral}
\end{align}
Then we have written an expression for the logarithm of the marginal
likelihood. This expression is still hard to be calculated analytically,
however from this equation we will be able to find estimators for
the logarithm of the marginal likelihood, and consequently for the
likelihood. To calculate these estimators, we will need to generate 
samples for a series of power posteriors of parameters, and this will
be explained in the next section.

% Subsection - Estimation of the Marginal Likelihood 
% -> We need to find samples of the posteriors
% -> Explain that we use the methodology of Xuan
% -> We use three Metropolis-Hastings
% -> A first burn-in step with jumps independent for each single 
% parameter. Adaptive Metropolis Hastings
% -> A second using a Variance-Covariance Matrix
% -> A third using populational MCMC
% TODO: talk about temperature: how the posterior is a "colder"
% distribution; 
% TODO: talk about how sampling from intermediate distributions is
% better than sampling directly from the posterior distribution
\subsection{Estimation of the Marginal Likelihood}
There are multiple approaches on estimating the 
Integral~\ref{eq:thermodynamic_integral} and, usually it is necessary to 
find samples of $p_{\beta_t} ({\bm \theta} | M, {\bm D})$ for a sequence 
of values of $\beta_t$ that vary from zero to one~\cite{Xura20, 
Vyshemirsky2007, Friel2008}. The methods that create such samples can 
take different approaches on the choice of the sequence $\beta_1 < 
\beta_2 < \ldots < \beta_T$, on the Metropolis-Hastings (MH) algorithms 
used, and finally on the estimator.

We are now going to show one possible methodology to estimate parameter 
values. First, we assume that there is a chosen sequence of $\beta_1 <
\beta_2 < \ldots < \beta_T$, so we can explain how to generate samples 
of power posterior distributions with these values of $\beta$. After
that we will explain how to choose the sequence of $\beta$ values and,
finally, present two possible estimators for $p (D | M)$.

\subsubsection{Power posteriors sampling}
\label{sec:power_posteriors_sampling}
Given a sequence of $\beta$ values, the method we present to sample from 
the power posteriors has three different steps, and all of them use
Metropolis-Hastings algorithms, similarly to what is done by Xu et al. 
(2010). For each of the $T$ $\beta$ values, the first two phases are
run independently, generating $T$ chains that are samples of the power 
posteriors. Then, on the third phase, each chain continues to grow, but
not independently, because two random chains will have their last 
accepted points swapped. This exchange leads to a mixture of chains of
samples of different power posteriors; this approach is called 
Populational Monte Carlo Markov Chain (Populational 
MCMC)~\cite{Friel2008}.

% We have to define names for the three different phases of the
% algorithm
%
% naive burn-in
% posterior shaped burn-in
% populational mcmc

The first step of sampling is run independently for each temperature and
we call it naive burn-in. The Metropolis-Hastings performed on this step
is started by taking a sample from the parameter priors. For each step,
the proposal distribution has to generate positive values and the
covariance matrix should be a diagonal covariance matrix, i.e. 
parameters are proposed independently. This covariance matrix is updated 
after every predefined number of iterations to adapt the proposal 
distribution according to the current sample. This type of MCMC 
algorithm, that updates its rules according to its trace is also called
adaptive MCMC algorithm. This update is performed as proposed in Gelman 
et al. (2013): 
%TODO: https://arxiv.org/pdf/1504.01896.pdf look for "that the
%acceptance rate should be close to"
% TODO: continuar aqui
\begin{itemize}
\item{if the acceptance rate of parameter points in the last 
    iterations is greater than $0.44$, then increase the variance of the
    jump;}
\item{if the acceptance rate is lower than $0.23$, then decrease the 
    variance of the jump.}
\end{itemize}
This rule is a golden rule for Metropolis-Hastings calibration since the
work of Roberts et al.~\cite{Roberts1997}. The rationale behind this
rule is that chains generated with small variance tend to need more 
steps to cover the space, while chains generated with larger variance
tend to reject more proposed parameters, staying at the same point of
the chain for many iterations.

Given that we are taking a sample from a power posterior with 
temperature $\beta_t$, the target function is  $p_{\beta_t} ({\bm
\theta})$. Hence, if the current point is ${\bm \theta}^{(t, i)}$, the 
probability of accepting a proposed parameter 
${\bm \theta}^* \sim J_{(t, i)} ({\bm \theta}^* | {\bm \theta}^{(t,
i)})$, is $\min (1, r)$ with
\begin{equation*}
    r = \frac{p_{\beta_t} ({\bm \theta}^*)}
             {p_{\beta_t} ({\bm \theta}^{(t, i)})}
        \frac{J_{(t, i)} ({\bm \theta}^{(t, i)} | {\bm \theta}^*)}
             {J_{(t, i)} ({\bm \theta}^* | {\bm \theta}^{(t, i)})},
\end{equation*}
%TODO: continue from here
with $J_{(t, i)}$ being the proposal jump distribution on iteration $i$ 
on chain of temperature $\beta_t$. Because sampling from $p_t(\theta | 
M, D)$ directly is not possible, and  since we defined in the beginning 
of the chapter that 
$p_{\beta_t} (\theta)~\propto~p (D|M, \theta)^{\beta_t} p (\theta | M)$,
we can rewrite this equation as:
\begin{equation}
    r = \frac{p (D | M, \theta^*)^{\beta_t}}
             {p (D | M, \theta^{(t, i)})^{\beta_t}}
        \frac{p (\theta^* | M)}
             {p (\theta^{(t, i)} | M)}
        \frac{J_{(t, i)} (\theta^{(t, i)} | \theta^*)}
             {J_{(t, i)} (\theta^* | \theta^{(t, i)})}.
    \label{eq:mh_ratio_step1}
\end{equation}

The second iteration also samples each temperature independently, and
differently from the first step, the parameters are not necessarily 
sampled independently. In fact, a portion of the last sampled parameters
on step one is used to create a covariance matrix, and this matrix is
then used as the covariance matrix of the first proposal distribution of 
the second step of sampling. After the first iteration, for every 
sampled parameter on the second step, the covariance matrix is updated 
to be the sample covariance of the selected points of step one plus the  
current points accepted on step two. The probability of accepting a 
proposed parameter on this step is the same  as it was on the first 
step, as described by Equation~\ref{eq:mh_ratio_step1}.

At the end of the second step, we will have $T$ chains of sampled 
parameters, containing the selected parameters of the first and second 
steps. For each of these chains there is a covariance matrix that was
being used as the covariance of the proposal distribution of the 
respective temperature. On step three, the covariance matrix is fixed 
and a Populational MCMC algorithm is performed. This algorithm continues 
the sampling for each temperature, now with a fixed covariance on the 
proposal distribution, and also mixes the samples of different 
temperatures in every iteration. This mix is achieved by swapping the 
last sampled elements of two random power posterior chains; the first of
the two chains chosen to swap, of temperature $\beta_i$, is chosen 
uniformly, while the second chain, of temperature $\beta_j$, is chosen 
following a Discrete Laplacian distribution given $\beta_i$, with 
probability function $p (j | i) \propto e^{|i - j| / 2}$. This algorithm 
was proposed by Friel et al. (2008) and can be summarized in the 
following steps:
\begin{enumerate}
    \item{For each temperature $\beta_t$, $t \in \{1, \ldots, T\}$, 
        update the $t$-th chain using MCMC with a proposal distribution
        that has the same covariance matrix used on the last iteration
        of temperature $\beta_t$ of the second step.}
    \item{Choose uniformly $i$ from $\{1, \ldots, T\}$. Then, choose
        $j$ with probability density function $p (j | i) \propto 
        e^{|i - j| / 2}$. Finally, swap the last sampled points of 
        the chains of temperature $\beta_i$ and $\beta_j$, which are
        $\theta^{(i, k)}$ and $\theta^{(j, k)}$, with probability 
        $\min\{1, r\}$, where: }

    \begin{equation*}
        r = \frac{p_{\beta_i}\left(\theta^{(j, k)}\right)}
             {p_{\beta_i}\left(\theta^{(i, k)}\right)}
        \frac{p_{\beta_j}\left(\theta^{(i, k)}\right)}
             {p_{\beta_j}\left(\theta^{(j, k)}\right)}
        \frac{p (j | i)}
             {p (i | j)}
    \end{equation*}
    which can be simplified to:
    \begin{equation*}
        r = \left[\frac{p \left(D | \theta^{(j, k)}\right)}
                {p \left(D | \theta^{(i, k)}\right)}\right]^{\beta_i}
            \left[\frac{p \left(D | \theta^{(i, k)}\right)}
                {p \left(D | \theta^{(j, k)}\right)}\right]^{\beta_j}
            \frac{p (j | i)}
                 {p (i | j)}
    \end{equation*}
\end{enumerate}

After finishing the third step of sampling, the samples obtained on the
first two phases are discarded and only the samples from the last phase
are used to estimate the marginal likelihood.

\subsubsection{Estimators of the Marginal Likelihood}
The sampling procedure explained on the 
Section~\ref{sec:power_posteriors_sampling} produces samples of the 
power posteriors $p_{\beta_1} (\theta), \ldots, p_{\beta_T} (\theta)$, 
and these samples can be used to estimate logarithm of the marginal 
likelihood:
\begin{equation}
    \ln p (D | M) = \int_0^1 \expectation_{p_\beta (\theta)} 
        [\ln p(D|\theta, M)]d\beta.
    \label{eq:marginal_likelihood_again}
\end{equation}
This can be achieved using both numerical integration or creating an
unbiased estimator. The choice of either approach of estimation will
imply in a method for choosing the sequence of temperatures $\beta_1, 
\ldots, \beta_T$. 

The method proposed by Friel et al. (2008), uses a numerical integration 
method to estimate the Integral~\ref{eq:marginal_likelihood_again}. 
Given that $T$ temperatures, $0 = \beta_1 < \beta_2 < \ldots < \beta_T = 
1$, were selected and samples of its respective power posteriors were 
generated, then using trapezoidal rule over the temperature allows us to 
estimate the integral as:
\begin{equation*}
    \log{p(D| M)} \approx \sum_{t = 0}^{T - 1} (\beta_{t + 1} - \beta_t)
\frac{
    \expectation_{p_{\beta_{t + 1}} (\theta)}[\log p(D | M, \theta)]
+ 
    \expectation_{p_{\beta_{t}} (\theta)}[\log p(D | M, \theta)]}
{2}
\end{equation*}
and if the sample of power posterior of temperature $\beta_t$ has $M_t$
parameter points, then we can rewrite this equation as:
\begin{equation}
\log{p(D| M)} \approx \sum_{t = 0}^{T - 1} (\beta_{t + 1} - \beta_t)
\frac{
    \frac{1}{M_{t + 1}}
    \sum_{i = 1}^{M_{t + 1}}  \log p(D | M, \theta^{(t + 1, i)})
+ 
    \frac{1}{M_t}
    \sum_{i = 1}^{M_t}  \log p(D | M, \theta^{(t, i)})}
{2}
\end{equation}
According to the work of Friel et al. (2008), a good temperature 
schedule for $\beta_1, \ldots, \beta_T$ that can be used in this 
approach is:
\begin{equation*}
    \beta_t = \left(\frac{t - 1}{T - 1}\right)^{c}, 
\end{equation*}
with $t \in {1, \ldots, T}$; with better results achieved when $T$ is
between $20$ and $100$ and $c$ is between $3$ and $5$.

Another method, proposed by Xu et al. (2010), considers that the 
temperature $\beta$ can be treated as a random variable, and therefore 
we can rewrite Integral~\ref{eq:marginal_likelihood_again} as:
\begin{equation}
    \expectation_{p (\beta)} 
        \left[\frac
            {\expectation_{p_\beta (\theta)}[\ln p(D|\theta, M)]}
            {p (\beta)}
        \right]
\end{equation}
The author uses this ideas to derive the following estimator. First, 
the interval $[0, 1]$ is discretized into $S - 1$ disjoint intervals 
$\Delta\beta_i = [t_{i + 1}, t_i]$ such that  $\sum_{i = 1}^{S - 1} 
(t_{i + 1} - t_{i}) = 1$. Then, for each interval $\Delta\beta_i$, $T_i$ 
temperatures are taken randomly from the uniform distribution on the 
interval $[t_{i + 1}, t_i]$. Finally, the estimator of the logarithm of
the marginal likelihood is given by:
\begin{equation}
    \sum_{s = 1}^{S - 1}\frac{|\Delta\beta_k|}
                             {T_k}
        \sum_{i = 1}^{M_k} \log p (D | M, \theta^{(\beta_{k, i})})
\end{equation}
where $\beta_{k, i}$ is the $i$-th sampled temperature from the interval
$\Delta\beta_k$; $\theta^{(\beta_{k, i})}$ is a parameter sampled 
from the power posterior $p_{\beta_{k, i}} (\theta)$; and 
$|\Delta\beta_k| = t_{k + 1} - t_k$. However, Xu et al. do not provide
information about the discretization method of the interval $[0, 1]$.


% ABC-SysBio
% What do I want to talk about ABC-SysBio?
% It is a likelihood-free approach for generating samples of a posterior
% distribution.
% Simply define the method: what does it do exactly?
% -> it approximates some posterior distribution without using a 
%    likelihood
% Give the generic steps of the algorithm
% Give as an example the ABC-Rejection algorithm
% Explain the result
% How would we get model ranking from this result?
% The ABC-SysBio Software
\section{Approximate Bayesian Computation}
\label{sec:abc_method}
Approximate Bayesian Computation (ABC) is an approach that allows 
generating samples from the posterior distribution without accessing the
likelihood function. Adding a model indicator parameter to parameters 
being sampled also allows us to create an approximate sample of the 
posterior $p (\theta, M | D)$, and from this sample it is possible to 
estimate $p (M | D)$, which can be used as a model ranking metric. The 
main idea of ABC methods is to generate a sample from posterior by 
generating parameter points that, when plugged into a model, generates 
simulations that dist from the experimental measurements at most by some
small $\epsilon$.

A generic ABC method that generates a sample of the posterior 
$p (\theta, m | D)$ is composed of the following steps:
\begin{enumerate}
    \item{Sample a parameter candidate $(\theta^*, M^*)$ from some 
        proposal distribution.} \label{alg:abc_step1}
    \item{Simulate the model $M^*$ with parameter values $\theta^*$, 
        generating simulated measurements $\phi (M^*, \theta^*) = D*$.}
    \item{Calculate, for some distance function $d$, the value of
        $d (D^*, D)$. If $d (D^*, D) < \epsilon$ for some previously
        specified $\epsilon$, then add $(\theta^*, M^*)$ to the sample.}
    \item{Repeat until some iteration limit.}
\end{enumerate}

The simplest ABC algorithm is the ABC Rejection, and it goes very 
similarly to the generic algorithm we just presented, with the 
specification that on step~\ref{alg:abc_step1} the proposal distribution
is the prior distribution. The output of this algorithm is a sample of 
the distribution $p (\theta, M| d (\phi (M, \theta), D) \leq \epsilon)$; 
when $\epsilon \to \infty$ this is then a sample of the prior 
distribution, and as $\epsilon \to 0$ then this sample tends to be a
sample of the posterior distribution~\cite{Pritchard1999}. This 
algorithm, however, does not perform well when the posterior 
distribution is very different from the prior distribution. For that 
reason, new ABC methods using Monte Carlo Markov Chains were 
created~\cite{Marjoram2003}. The ABC MCMC method proposed by Marjoram et 
al. (2003) produces a Markov Chain whose stationary distribution is
$p (\theta, M | d (\phi (M, \theta), D) \leq \epsilon)$. Nonetheless, 
this algorithm might still suffer from correlation in samples or even
get stuck in regions of local peaks of probability. For that reason,
the ABC sequantial Monte Carlo (ABC SMC)method was 
proposed~\cite{Toni2009}.

%-> Explain ABC SMC
The ABC SMC method creates a sequence of samples with the goal of 
getting closer to a posterior sample in each step. Let we simplify the
notation by saying that $d(\phi (M, \theta), D)$ is just 
$\rho_{M, \theta}$. From a predefined sequence $\epsilon_1, \ldots, 
\epsilon_T$ the algorithm generates a sequence of samples that 
represents the distributions 
$p (\theta, M| \rho_{M, \theta} \leq \epsilon_1), 
p (\theta, M| \rho_{M, \theta} \leq \epsilon_2), \ldots,
p (\theta, M| \rho_{M, \theta} \leq \epsilon_T)$. At the first 
generation, a sample of 
$p (\theta, M| \rho_{M, \theta} \leq \epsilon_1)$ is created by 
proposing points from the prior distribution. Then, for next generations
the candidates to the sample are proposed based on the points of the 
last generation and their weight, plus some noise determined by a 
perturbation Kernel; the weight of a point $(\theta^*, M^*)$ on 
generation $t$ is a estimative of  
$p (\theta = \theta^*, M = M^* | \rho_{M, \theta} \leq \epsilon_t)$. The 
pseudo-code~\ref{code:abc_smc} presents the ABC SMC algorithm.

\begin{algorithm}[h]
\textsc{ABC SMC} $(\mathcal{M}, D)$
\begin{algorithmic}[1]
    \State Define the sequence $\epsilon_1, \ldots, \epsilon_T$.
    \State Define $N$, the sample size for each generation. 
    \State Sample $\{(\theta^{(1, 1)}, M^{(1, 1)}), 
                     (\theta^{(1, 2)}, M^{(1, 2)}), \ldots, 
                     (\theta^{(1, N)}, M^{(1, N)})\}$ from 
                     $p (\theta, M| D)$.
    \State Set $w^{(1, i)} = 1, \forall i \in {1, \ldots, N}$.  
    \For{$t \in \{1, \ldots, T\}$}
        \State $i \gets 1$
        \While{$i \leq N$}
            \State Sample $M^*$ from $p (M | D)$, the model prior.
            \State Sample $(\theta^{(t - 1, k)}, M^*)$ from the 
                last generation with weight $w^{(t - 1, k)}$.
            \State Create $(\theta^*, M^*)$ by perturbing 
                $\theta^{(t - 1, k)}$; 
                $\theta^* \propto K^t(\theta | \theta^{(t - 1, k)})$.
            \If{$p (\theta^* | M^*) = 0$}
                \State Continue to next iteration.
            \EndIf
            \State $D* \gets \phi (M^*, \theta^*)$
            \If{$d (D^*, D) \leq \epsilon$}
                \State $i \gets i + 1$
                \State $(\theta^{(t, i)}, M^{(t, i)}) \gets 
                    (\theta^*, M^*)$
            \EndIf
        \EndWhile
        \State Calculate the weights of the population:
            %\begin{equation*}
$                w^{(t, i)} = \frac{p (\theta^{(t, i)} | M^{(t, i)})}
                         {\sum_{j = 1}^N w^{(t-1, j)}p_{K^t}
                            (\theta^{(t-1, j)}, \theta^{(t, i)})}$
            %\end{equation*}
    \EndFor
    \Return
\end{algorithmic}
\caption{Pseudo-code of ABC SMC.}
\label{code:abc_smc}
\end{algorithm}
The ABC SMC algorithm is implemented in Python language in a software called 
ABC-SysBio~\cite{Liepe2014}.

