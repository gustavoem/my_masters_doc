%begin-include
In this chapter we present the development process and implementation
details of the SigNetMS software, which stands for {\bf Sig}naling 
{\bf Net}work {\bf M}odel {\bf S}election. This software is capable of
producing an estimative of the marginal likelihood of a model given
experimental data, $p({\bm D} | M)$.

% What are we going to talk about in this chapter?
% - first, we have to talk about which methodology this software uses
%   -> make sure you citate BioBayes and how its cumbersome to use it
% to create a model ranking. we should state that we are using marginal
% likelihood estimates here.
%   -> make sure we specify the input and output of this software
%   -> include program arguments
% - then, we can talk about the implementation and optimizations
%   -> how did we implement the sampling? what was the used proposal
%   distribution?
%   -> fast integration of system of differential equations
%   -> parallel sampling
%   -> running the algorithm on a cluster

\section{The SigNetMS Software}
% General Aspects of the software

% The Bayesian methodology
% - SigNeMS creates an estimative of the marginal likelihood of a model
% given a set of experimental data.
% - To create this estimative, SigNetMS uses ideas of Thermodynamic
%   Integration.
% - There is a software that can carry similar simulations, called
%   BioBayes, however we found its use cumbersome, mainly because:
%   it is a GUI software, which does not fit our type of applications,
%   that should be ran in a server for several hours. Moreover, we
%   intended to link the marginal likelihood output to other programs,
%   for model selection.

% Details on how the program creates the system of ODEs
% Details on how the proram estimates the likelihood

SigNetMS is a Python program that can be used as a tool for model
selection. The source code is available on 
Github\footnote{https://github.com/gustavoem/SigNetMS} and it is open
source, under the GNU General Public License.

This program expects as the input: a signaling pathway model,
represented by a Systems Biology Markup Language
(SBML)~\cite{hucka2003systems} file, with the definition of reactions,
kinetic laws and initial concentrations of chemical species; an
Extensible Markup Language (XML) file with experimental data, including 
time series measurements of the  biological phenomena of interest;
another XML file with definitions of prior distributions of reaction
rate constants; and, finally, a set of parameter values that determine
the sampling process of model parameters. There are also optional
parameters on SigNetMS, used to control random number generator seeds,
number of execution threads, and verbose runs.

The output of the program is composed by an estimative of the marginal
likelihood of the model, given experimental data, $p({\bm D} | M)$ and a
list of parameter values $({\bm \theta}^1, {\bm \theta}^2, \ldots {\bm
\theta}^l)$ that represent a sample of the distribution $p({\bm \theta}
| M, {\bm D})$. If one simulates the model $M$ with parameter values
from the sample, we expect that, the higher the marginal likelihood, the
closer the simulation is to the experimental data.

To calculate this estimative, we use the ideas of Thermodynamic
Integration we presented on section~\ref{sec:thermodynamic_integration}.
Before implementing SigNetMS we also considered using another software,
called BioBayes~\cite{Vyshemirsky2008}, that has a very similar approach
to produce an estimative of the marignal likelihood. However, we did not
proceed to use this software because there was only a graphical user
interface version of the software, which made the process of running
instances and collecting results cumbersome. We have also tried
contacting the authors to obtain the source code, however they could not
provide us that.

\section{Creating an estimative of the marginal likelihood}
% Since we are using a Thermodynamic Integration  approach, we need to 
% define a likelihood function, create samples of power posterior 
% distributions and, finally define an approximation to the marginal
% likelihood.

% The likelihood function...
% The samples of power posteriors are...
% The approximation of the marginal likelihood are...
% The whole process, has the following order:
% -> all inputs are read;
%   -> The sbml model is read and transformed into a System of ODE
%       -> this model can determine a simulation, given a list of
%       parameter values and a measurement unit;
%   -> Parameter priors, experimental data are read and saved
% -> sampling of posterior starts with naive sampling
% -> then we proceed to covariance shaped burn-in
% -> then we go to populational monte carlo markov chain
% -> then we use the approximation to generate the margginal likelihood

To create estimates of the marginal likelihood using the Thermodynamic
Integration approach, we need to define a likelihood function $p({\bm
D}| M, {\bm \theta})$ and also samples power posterior distributions
$p_\beta({\bm \theta})$ for a sequence of values of $\beta$. On
SigNetMS, we use the trapezoidal rule to approximate the marginal
likelihood, given by the
equation~\ref{eq:marginal_likelihood_trapezoidal_approximation}. 
Inspired by the work of Friel et al., we discretize the
interval $[0, 1]$ of power posteriors using the $\beta_i =
\left(\frac{i - 1}{T - 1}\right)^c$, where $T = 20$, $c = 4$ and $i \in
\{1, 2, \ldots, T\}$.

% TODO: say here that we used pipipi popopo beta values and that with
% samples of those power posteriors, we estimated the marginal
% likelihood as...

\subsection{Implementing the likelilhood function}
%-> The likelihood function is...
%-> To implement this, we must simulate the model with parameter values
%of theta.
%-> these simulations involves the numerical integration of a system of 
% ordinary differential equaitons
%-> current methods of numerical integrations are iterative and can be
% very consty depending on the size of the numerical system and also on
% the type of the system, stiff or not
The likelihood function we used is the same as we defined in
equation~\ref{eq:likelihood_multivariate}:
\begin{equation*}
    p ({\bm D} | M,{\bm \theta}) = 
        p_{\mathcal{N}_{\left(\vec{0}, \Sigma\right)}}
        (\phi (M, {\bm\theta}) - {\bm D}),
\end{equation*}
where ${\bm D}$ is the experimental measurement, $M$ is the model,
${\bm \theta}$ is a set of parameter values, $\Sigma$ is the variance of
the error of experimental observations, and, finally, $\phi$ is a 
function that calculates an approximation of the results of the
experiment that produced ${\bm D}$, applied to the simulated environment
of model $M$ with parameter values ${\bm \theta}$. This simulation of
the model is created by deriving a system of ordinary differential
equations, and then numerically integrating this system, over the time
steps defined by the experimental measurements. To reproduce the same
experiment that generated ${\bm D}$, SigNetMS expects that the XML file 
containing experimental data also contains a mathematical representation
of which quantity was measured, in terms of concentrations of chemical
species of the system.

% Numerical integration of systems are solved using iterative methods
% stiff non stiff
% we used  third-party software to solve this problem
The numerical integration of the system is alone a hard problem, and
therefore, we used third-party software to produce such integrations.
The most popular software available for this problem conduce
iterative algorithms that, step by step, aproximate the state of the
system for a time interval. It is important to know that some instances
of the problem can be stiff, meaning that they may make the integration
algorithm be unstable, since it may need consecutive iterations in 
really small steps. Since we did not have time to go in details of when 
such cases occur, we chose third-party softwares that can adapt the used
algorithm according to the stiffnes of the instance.

After the implementation of the function $\phi$, most of the work to
implement the likelihood function is done. The reamining work is to
calculate the value of the probability density function
$p_{\mathcal{N}_{\left(\vec{0}, \Sigma\right)}}$ and that can easily be
accomplished using statistical packages such as
SciPy~\cite{2020SciPy-NMeth}.

\subsection{Sampling parameters from power-posteriors}
% after implementing the likelihood function, we can create the samples
% of the power posteriors;
% -> we used the same methodology we presented before, this methodology
%  is based on using variations of the metropolis-hastings algorithm,

After implementing the likelihood function, we can move to the creation
of samples of power posteriors. The methodology we used to generate such
samples is identical to the one we presented on
section~\ref{sec:estimation_of_marginal_likelihood}. And for this
reason, we divided the sampling process in three phases: naive burn-in,
adaptive burn-in and Populational MCMC. All of these process are types
of a Metropolis-Hastings procedure. The number of iterations of each
phase is determined by SigNetMS's arguments, and each phase has a
different scheme to determine the proposal distribution.

On the first phase, we start the sample of every chain (for each value
of $\beta$) with a random draw from the prior distribution of
parameters. Before the first iteration, we also create an estimative of
the variance of the logarithm of each model parameter, independently.
These estimates are used to build the first covariance matrix of the
jumping distribution; we use a diagonal matrix where the diagonal
elements are set as the estimated variance of the logarithm scaled
sample of the associated parameter. This matrix is rescaled according to 
the acceptance rate, as described in
section~\ref{sec:estimation_of_marginal_likelihood} after a number of
iterations that is defined in one of arguments of SigNetMS. For each
iteration, we determine that the jumping distribution is a multivariate
lognormal $({\bm \mu}, \Sigma)$ distribution, with covariance matrix as
explained before, and with ${\bm \mu} = \log_e({\bm \theta}^t)$, where
${\bm \theta}^t$ is the current sample point. That means we take a
sample ${\bm X}$ of the multivariate normal $\mathcal{N}({\bm \mu},
\Sigma)$ and then we set our sampled value  as ${\bm Y} = \exp({\bm
X})$, which is a standard procedure to produce samples of lognormal
distributions.

On the second phase, the posterior shaped burn-in phase, we also use a 
lognormal as the proposal distribution, however the covariance matrix is 
not diagonal. Half of the sample produced in the first phase is
discarded, and for each step, we calculate the covariance of the
log-scaled current sample, producing the matrix used as the covariance
matrix for the proposal distribution. Similarly to the first phase, the
proposal distribution also uses ${\bm \mu} = \log_e({\bm \theta}^t)$. It
is important to remember that up to the end of this sampling phase, each
power posterior sample is produced independently.

The third and last phase, we perform a Populational Monte Carlo Markov
Chain. In this procedure, we iterate each chain of power posterior
samples using the same algorithm of the second phase (except we do not
update the covariance matrix anymore), followed by an exchange the last
sampled points on two random selected power posteriors. At the end of
this phase, we discard parameters sampled on previous phases and we set
the actual sample as all the parameters sampled in this phase.
