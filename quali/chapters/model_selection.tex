%begin-include

In this chapter we will present two state of the art methodologies that 
can be used to rank models, both of them are Bayesian approaches. The
first approach is to estimate the marginal likelihood of the data $D$ 
being reproduced by a model $M$, $p (D | M)$. The estimation of this 
probability is done by taking samples of tempered posterior 
distributions of model parameters; these tempered posterior 
distributions bridge the prior and posterior parameter distributions and 
allow a better estimation of the marginal likelihood. Another method,
likelihood-free, is to use Approximate Bayesian Computation; this method
allows us to estimate the probability $p (M | D)$ in a simpler 
algorithm.

\section{Model ranking using Marginal Likelihood}
% Simple explanation of this model ranking
% Describe the likelihood function 
% However, this is very hard to calculate
% It is possible to apply an Importance Sampling Estimator {cite 
% Newton and Raftery}. However, it was showed on {cite girolami again}
% that these methods do not perform well.
% Then it was proposed to use Thermodynamic Integration. Make it clear 
% that this allows us to create some estimators.
% Subsection - Thermodynamic Integration
% -> explain how to derive it
%   -> name what is the power posterior
% -> include here: how to estimate it?
% Subsection - Estimation of the Marginal Likelihood 
% -> We need to find samples of the posteriors
% -> Explain that we use the methodology of Xuan
% -> We use three Metropolis-Hastings
% -> A first burn-in step with jumps independent for each single 
% parameter. Adaptive Metropolis Hastings
% -> A second using a Variance-Covariance Matrix
% -> A third using populational MCMC
The marginal likelihood of an experiment measurement $D$ being 
reproduced by a model $M$, $p (D | M)$, can be used as a model ranking 
metric as it determines which model makes the experimental observations 
more likely to happen. Before defining how to calculate the marginal 
likelihood, we must define what the likelihood function is. To calculate 
the likelihood $p (D | M, \theta)$, we must understand that conditioning 
the observation to the model and parameters means that in 
the probability space from which $D$ is taken, the model $M$ is the 
``real'' model and it has the parameter values of $\theta$; i.e. the 
model $M$ with parameters $\theta$ controls the behaviour of the system 
from which $D$ was observed. Then, assuming that the observations have a 
Gaussian error, and that they are taken in a time series of $m$ time 
steps, we can define the likelihood as:
\begin{equation}
    p (D | M, \theta) = p_{\mathcal{N}_{\left(\vec{0}, \Sigma\right)}}
        (\phi (M,\theta) - D),
\label{eq:likelihood_multivariate}
\end{equation}
where $\phi (M, \theta) \in \fieldR^m$ is the experimental measurement 
on the simulation generated by the model $M$ with parameters $\theta$,
and \smash{$p_{\mathcal{N}_{(\vec{0}, \Sigma)}} (\cdot)$} is the 
probability density function of a Multivariate Normal variable with mean 
$\vec{0}$ and covariance matrix $\Sigma$. As a matter of fact, as it is
done in the work of Xu et al. (2010), we can consider that the 
observation error is independent for each time step~\cite{Xura20}, 
therefore we can simplify~\ref{eq:likelihood_multivariate} to:
\begin{equation}
    p (D | M, \theta) = \prod_{i = 1}^m p_{\mathcal{N}_{\left(0, 
        \sigma^2\right)}} (\phi_i (M,\theta) - D_i).
\label{eq:likelihood}
\end{equation}
The $\sigma^2$ used in equation~\ref{eq:likelihood} is also a parameter
of the model, which means that, for some $k$, $\theta_k = \sigma^2$.

Now that we defined the likelihood function, we can write the marginal 
likelihood as:
\begin{equation}
    p (D | M) = \int_{\Theta} p (D | M, \theta) p (\theta | M)d\theta.
\label{eq:marginal_likelihood}
\end{equation}
However, calculating this integral analytically is only possible in 
very special cases and, usually, it would depend on knowing models for 
the distributions associated to these probability functions, which is 
generally not possible in our case.

Even though this integral is very hard to be calculated, there are 
methods that allow us to estimate its value. A straight forward method 
to estimate this integral value is using Importance Sampling 
Estimators~\cite{Newton1993}. This method uses the Monte Carlo integral 
estimation method that can estimate integrals of the form 
$\int g(\lambda) p(\lambda)d\lambda$ using the estimator:
\begin{equation*}
    \hat{I} = \sum_{i = 1}^m w_i g(\lambda_i) / \sum_{i = 1}^m w_i,
\label{eq:importance_sampling_estimator}
\end{equation*}
where $w_i = p (\lambda) / p^* (\lambda)$, and $p^*(\cdot)$ is known as 
the importance sampling function. If we set $\lambda = \theta | M$ and
use the prior ($p(\theta | M)$) or the posterior ($p(\theta | M, D)$) as 
importance sampling functions, then we would get respectively the 
estimators:
\begin{equation*}
\begin{aligned}
    \frac{1}{m} \sum_{i = 1}^m p(D|M, \theta^{(i)}) &&& 
        (\text{with } \theta^{(i)} \sim p(\theta|M)), \\
    \left(\frac{1}{m} \sum_{i = 1}^m p(D|M, \theta^{(i)})^{-1} \right)^{-1} &&&
        (\text{with } \theta^{(i)} \sim p(\theta|M, D)).
\end{aligned}
\end{equation*}
However, as showed by Vyshemirsky et al. (2007), these estimators might
produce very large variances and may not perform well for biochemical
model selection applications. Hence, new methods with ideas of 
thermodynamics were proposed. These methods are based on rewriting the
marginal likelihood equation using intermediate distributions of 
parameters between the prior and posterior 
distributions~\cite{Friel2008}.

% Subsection - Thermodynamic Integration
% -> explain how to derive it
%   -> name what is the power posterior
% -> include here: how to estimate it?
\subsection{Thermodynamic Integration for Marginal Likelihood}
The Thermodynamic Integration is a method that proposes to rewrite
the integral~\ref{eq:marginal_likelihood} using ideas of thermodynamics,
providing new estimators for the marginal likelihood. This method is
able to rewrite the marginal likelihood integral in a way that it 
marginalizes the likelihood through many intermediate probability spaces 
of parameters, bridging the prior and posterior distributions of 
parameters. These distributions are also called tempered distributions
or power posteriors~\cite{Friel2008}.

Given a parameter prior distribution $p (\theta | M)$ and a posterior 
distribution $p (\theta | D, M)$, then we define a power posterior 
distribution with temperature $\beta$ as:
\begin{equation*}
    p_{\beta} (\theta) = \frac{p (D | \theta, M)^\beta p(\theta | M)}
                              {z (\beta)},
\end{equation*}
where
\begin{equation*}
    z (\beta) = \int_\Theta p (D | \theta, M)^\beta p(\theta | M) 
            d\theta.
\end{equation*}
Note that when $\beta = 0$, then $p_{\beta=0} = p (\theta | M)$, the 
prior distribution of the parameters; also, when $\beta = 1$, then
\begin{equation*}
    p_{\beta=1}(\theta) = \frac{p (D | \theta, M) p(\theta | M)}
                         {z (\beta)}
                        =\frac{p (D, \theta|M)}
                              {\int_\Theta p (D, \theta | M)d\theta}
                        =\frac{p(\theta | D, M) p(D|M)}{p (D | M)}
                        =p (\theta | D, M),
\end{equation*}
the posterior distributions of the parameters.

Now, let we consider the derivative of $\ln z(\beta)$.
\begin{align}
    \frac{d}{d\beta} \ln z(\beta) &= \frac{1}{z(\beta)}  
        \frac{d}{d\beta} z(\beta) \notag\\
    &= \frac{1}{z(\beta)} \frac{d}{d\beta} 
        \int_\Theta p (D | \theta, M)^\beta p(\theta | M) d\theta \notag\\
    \shortintertext{(using that $\frac{d}{dx} c^x = c^x \ln c$)}
    &= \frac{1}{z(\beta)} \int_\Theta p (D | \theta, M)^\beta 
        p(\theta | M) \ln p(D|\theta, M)d\theta \notag\\
    &= \int_\Theta \frac{p (D | \theta, M)^\beta p(\theta | M)}
                        {z(\beta)}
                    \ln p(D| \theta, M)d\theta \notag\\
    &=\int_\Theta p_\beta (\theta) \ln p (D | \theta, M)d\theta \notag\\ 
    &=\expectation_{p_\beta (\theta)} [\ln p(D|\theta, M)]. \label{eq:z_derivative}
\end{align}
And it is not hard to see that:
\begin{equation}
\begin{gathered}
    z (0) = \int_\Theta p (\theta | M)d\theta = 1 \\
    z (1) = \int_\Theta p (D, \theta | M)d\theta = p (D|M) 
    \label{eq:z_on_limits}
\end{gathered}
\end{equation}

Using equations~\ref{eq:z_on_limits} and equality~\ref{eq:z_derivative}
we can write that:
\begin{align}
    \int_0^1 \expectation_{p_\beta (\theta)} [\ln p(D|\theta, M)]d\beta 
    &= \int_0^1 \frac{d}{d\beta} \ln z(\beta) d\beta \notag \\
    &= \Big[\ln z(\beta)\Big]\bigg\rvert^1_0 \notag \\
    &= \ln p (D | M).
    \label{eq:thermodynamic_integral}
\end{align}
Then we have written an expression for the logarithm of the marginal
likelihood. This expression is still hard to be calculated analytically,
however from this equation we will be able to find estimators for
the logarithm of the marginal likelihood, and consequently for the
likelihood. To calculate this estimators, we will need to generate 
samples for a series of power posteriors of parameters, and this will
be explained in the next section.

% Subsection - Estimation of the Marginal Likelihood 
% -> We need to find samples of the posteriors
% -> Explain that we use the methodology of Xuan
% -> We use three Metropolis-Hastings
% -> A first burn-in step with jumps independent for each single 
% parameter. Adaptive Metropolis Hastings
% -> A second using a Variance-Covariance Matrix
% -> A third using populational MCMC
\subsection{Estimation of the Marginal Likelihood}
There are multiple approaches on estimating the 
integral~\ref{eq:thermodynamic_integral}, and for all of them, it is 
necessary to find samples of $p_{\beta_t} (\theta | M, D)$ for a 
sequence of values of $\beta_t$ that vary from zero to one~\cite{Xura20, 
Vyshemirsky2007, Friel2008}. The differences on these methods are mainly 
on the choice of the sequence $\{\beta_1, \beta_2, \ldots, \beta_T\}$,
on the Metropolis-Hastings (MH) algorithms used, and finally on the 
estimator.

We are now going to show a possible methodology to estimate parameter 
values. First, we assume that there is a chosen sequence of 
$\{\beta_1, \beta_2, \ldots, \beta_T\}$ to explain how to generate 
samples of power posterior distributions with these temperatures. Then, 
we explain how to choose these values and present two possible 
estimators for $p (D | M)$.

\subsubsection{Power posteriors sampling}
\label{sec:power_posteriors_sampling}
Given a sequence of temperatures, the method used to sample from the
power posteriors has three different steps, all of them using 
Metropolis-Hastings algorithms, similarly to what is done by Xu et al. 
(2010). For each of the $T$ temperatures, the first two phases are
run separately, generating $T$ chains that are samples of the power 
posteriors. Then, on the third phase, each chain continues to grow, but
not independently because two random chains will have their last accepted 
point swapped, what causes the chains to be mixed; this approach on 
mixing these chains is called Populational Monte Carlo Markov 
Chain (Populational MCMC)~\cite{Friel2008}.

The first step of sampling is run independently for each temperature. 
The Metropolis-Hastings performed on this step is started taking a 
sample from the parameter priors. The proposal distribution is 
a multivariate Gaussian that has mean approximately equal to the last 
point sampled and with a diagonal covariance matrix, i.e. parameters are 
proposed independently. This covariance matrix is updated repeatedly 
after a predefined number of iterations with the goal of adapting the 
proposal distribution to the parameter space. This update is performed 
as proposed in Gelman et al. (2013): 
\begin{itemize}
\item{if the acceptance rate of parameter points in the last 
    iterations is greater than $0.44$, then increase the variance of the
    jump;}
\item{if the acceptance rate is lower than $0.23$, then decrease the 
    variance of the jump.}
\end{itemize}
Given that we are taking a sample from a power posterior with 
temperature $\beta_t$, the target function is 
$p_{\beta_t} (\theta)$. Hence, if the current point 
is $\theta^{(t, i)}$, the probability of accepting a proposed parameter 
$\theta^* \sim J_{(t, i)} (\theta^* | \theta^{(t, i)})$, is 
$\min (1, r)$ with
\begin{equation*}
    r = \frac{p_{\beta_t} (\theta^*)}
             {p_{\beta_t} (\theta^{(t, i)})}
        \frac{J_{(t, i)} (\theta^{(t, i)} | \theta^*)}
             {J_{(t, i)} (\theta^* | \theta^{(t, i)})},
\end{equation*}
with $J_{(t, i)}$ being the proposal jump distribution on iteration $i$ 
on chain of temperature $\beta_t$. Because sampling from $p_t(\theta | 
M, D)$ directly is not possible, and  since we defined in the beginning 
of the chapter that 
$p_{\beta_t} (\theta)~\propto~p (D|M, \theta)^{\beta_t} p (\theta | M)$,
we can rewrite this equation as:
\begin{equation}
    r = \frac{p (D | M, \theta^*)^{\beta_t}}
             {p (D | M, \theta^{(t, i)})^{\beta_t}}
        \frac{p (\theta^* | M)}
             {p (\theta^{(t, i)} | M)}
        \frac{J_{(t, i)} (\theta^{(t, i)} | \theta^*)}
             {J_{(t, i)} (\theta^* | \theta^{(t, i)})}.
    \label{eq:mh_ratio_step1}
\end{equation}

The second iteration also samples each temperature independently, and
differently from the first step, the parameters are not necessarily 
sampled independently. In fact, a portion of the last sampled parameters
on step one is used to create a covariance matrix, and this matrix is
then used as the covariance matrix of the first proposal distribution of 
the second step of sampling. After the first iteration, for every 
sampled parameter on the second step, the covariance matrix is updated 
to be the sample covariance of the selected points of step one plus the  
current points accepted on step two. The probability of accepting a 
proposed parameter on this step is the same  as it was on the first 
step, as described by equation~\ref{eq:mh_ratio_step1}.

At the end of the second step, we will have $T$ chains of sampled 
parameters, containing the selected parameters of the first and second 
steps. For each of these chains there is a covariance matrix that was
being used as the covariance of the proposal distribution of the 
respective temperature. On step three, the covariance matrix is fixed 
and a Populational MCMC algorithm is performed. This algorithm continues 
the sampling for each temperature, now with a fixed covariance on the 
proposal distribution, and also mixes the samples of different 
temperatures in every iteration. This mix is achieved by swapping the 
last sampled elements of two random power posterior chains; the first of
the two chains chosen to swap, of temperature $\beta_i$, is chosen 
uniformly, while the second chain, of temperature $\beta_j$, is chosen 
following a Discrete Laplacian distribution given $\beta_i$, with 
probability function $p (j | i) \propto e^{|i - j| / 2}$. This algorithm 
was proposed by Friel et al. (2008) and can be summarized in the 
following steps:
\begin{enumerate}
    \item{For each temperature $\beta_t$, $t \in \{1, \ldots, T\}$, 
        update the $t$-th chain using MCMC with a proposal distribution
        that has the same covariance matrix used on the last iteration
        of temperature $\beta_t$ of the second step.}
    \item{Choose uniformly $i$ from $\{1, \ldots, T\}$. Then, choose
        $j$ with probability density function $p (j | i) \propto 
        e^{|i - j| / 2}$. Finally, swap the last sampled points of 
        the chains of temperature $\beta_i$ and $\beta_j$, which are
        $\theta^{(i, k)}$ and $\theta^{(j, k)}$, with probability 
        $\min\{1, r\}$, where: }

    \begin{equation*}
        r = \frac{p_{\beta_i}\left(\theta^{(j, k)}\right)}
             {p_{\beta_i}\left(\theta^{(i, k)}\right)}
        \frac{p_{\beta_j}\left(\theta^{(i, k)}\right)}
             {p_{\beta_j}\left(\theta^{(j, k)}\right)}
        \frac{p (j | i)}
             {p (i | j)}
    \end{equation*}
    which can be simplified to:
    \begin{equation*}
        r = \left[\frac{p \left(D | \theta^{(j, k)}\right)}
                {p \left(D | \theta^{(i, k)}\right)}\right]^{\beta_i}
            \left[\frac{p \left(D | \theta^{(i, k)}\right)}
                {p \left(D | \theta^{(j, k)}\right)}\right]^{\beta_j}
            \frac{p (j | i)}
                 {p (i | j)}
    \end{equation*}
\end{enumerate}

After finishing the third step of sampling, the samples obtained on the
first two phases are discarded and only the samples from the last phase
are used to estimate the marginal likelihood.

\subsubsection{Estimators of the Marginal Likelihood}
The sampling procedure explained on the 
section~\ref{sec:power_posteriors_sampling} produces samples of the 
power posteriors $p_{\beta_1} (\theta), \ldots, p_{\beta_T} (\theta)$, 
and this samples can be used to estimate logarithm of the marginal 
likelihood:
\begin{equation}
    \ln p (D | M) = \int_0^1 \expectation_{p_\beta (\theta)} 
        [\ln p(D|\theta, M)]d\beta.
    \label{eq:marginal_likelihood_again}
\end{equation}
And this can be achieved using both numerical integration or creating an
unbiased estimator. The choice of either approach of estimation will
imply in a method for choosing the sequence of temperatures $\beta_1, 
\ldots, \beta_T$. 

The method proposed by Friel et al. (2008), uses a numerical integration 
method to estimate the integral~\ref{eq:marginal_likelihood_again}. 
Given that $T$ temperatures, $0 = \beta_1 < \beta_2 < \ldots < \beta_T = 
1$, were selected and samples of its respective power posteriors were 
generated, then using trapezoidal rule over the temperature allows us to 
estimate the integral as:
\begin{equation*}
    \log{p(D| M)} \approx \sum_{t = 0}^{T - 1} (\beta_{t + 1} - \beta_t)
\frac{
    \expectation_{p_{\beta_{t + 1}} (\theta)}[\log p(D | M, \theta)]
+ 
    \expectation_{p_{\beta_{t}} (\theta)}[\log p(D | M, \theta)]}
{2}
\end{equation*}
and if the sample of power posterior of temperature $\beta_t$ has $M_t$
parameter points, then we can rewrite this equation as:
\begin{equation}
\log{p(D| M)} \approx \sum_{t = 0}^{T - 1} (\beta_{t + 1} - \beta_t)
\frac{
    \frac{1}{M_{t + 1}}
    \sum_{i = 1}^{M_{t + 1}}  \log p(D | M, \theta^{(t + 1, i)})
+ 
    \frac{1}{M_t}
    \sum_{i = 1}^{M_t}  \log p(D | M, \theta^{(t, i)})}
{2}
\end{equation}
According to the work of Friel et al. (2008), a good temperature 
schedule for $\beta_1, \ldots, \beta_T$ that can be used in this 
approach is:
\begin{equation*}
    \beta_t = \left(\frac{t - 1}{T - 1}\right)^{c}, 
\end{equation*}
with $t \in {1, \ldots, T}$; with better results achieved when $n$ is
between $20$ and $100$ and $c$ is between $3$ and $5$.

Another method, proposed by Xu et al. (2010), considers that the 
temperature $\beta$ can be treated as a random variable, and therefore 
we can rewrite integral~\ref{eq:marginal_likelihood_again} as:
\begin{equation}
    \expectation_{p (\beta)} 
        \left[\frac
            {\expectation_{p_\beta (\theta)}[\ln p(D|\theta, M)]}
            {p (\beta)}
        \right]
\end{equation}
The author uses this ideas to derive the following estimator. First, 
the interval $[0, 1]$ is discretized into $S - 1$ disjoint intervals 
$\Delta\beta_i = [t_{i + 1}, t_i]$ such that  $\sum_{i = 1}^{S - 1} 
(t_{i + 1} - t_{i}) = 1$. Then, for each interval $\Delta\beta_i$, $T_i$ 
temperatures are taken randomly from the uniform distribution on the 
interval $[t_{i + 1}, t_i]$. Finally, the estimator of the logarithm of
the marginal likelihood is given by:
\begin{equation}
    \sum_{s = 1}^{S - 1}\frac{|\Delta\beta_k|}
                             {T_k}
        \sum_{i = 1}^{M_k} \log p (D | M, \theta^{(\beta_{k, i})})
\end{equation}
where $\beta_{k, i}$ is the $i$-th sampled temperature from the interval
$\Delta\beta_k$; $\theta^{(\beta_{k, i})}$ is a parameter sampled 
from the power posterior $p_{\beta_{k, i}} (\theta)$; and 
$|\Delta\beta_k| = t_{k + 1} - t_k$.

%\subsection{Implementation of SigNetMS}
% -> Python
% -> Uses the libSBML python package


% ABC-SysBio
% What do I want to talk about ABC-SysBio?
% It is a likelihood-free approach for generating samples of a posterior
% distribution.
% Simply define the method: what does it do exactly?
% -> it approximates some posterior distribution without using a 
%    likelihood
% Give the generic steps of the algorithm
% Give as an example the ABC-Rejection algorithm
% Explain the result
% How would we get model ranking from this result?
% The ABC-SysBio Software
\section{Approximate Bayesian Computation}
Approximate Bayesian Computation (ABC) is an approach that allows 
generating samples from the posterior distribution without accessing the
likelihood function. Adding a model indicator parameter to parameters 
being sampled also allows us to create an approximate sample of the 
posterior $p (\theta, M | D)$, and from this sample it is possible to 
estimate $p (M | D)$, which can be used as a model ranking metric. The 
main idea of ABC methods is to generate a sample from posterior by 
generating parameter points that, when plugged into a model, generates 
simulations that dist from the experimental measurements at most by some
small $\epsilon$.

A generic ABC method that generates a sample of the posterior 
$p (\theta, m | D)$ is composed of the following steps:
\begin{enumerate}
    \item{Sample a parameter candidate $(\theta^*, M^*)$ from some 
        proposal distribution.} \label{alg:abc_step1}
    \item{Simulate the model $M^*$ with parameter values $\theta^*$, 
        generating simulated measurements $\phi (M^*, \theta^*) = D*$.}
    \item{Calculate, for some distance function $d$, the value of
        $d (D^*, D)$. If $d (D^*, D) < \epsilon$ for some previously
        specified $\epsilon$, then add $(\theta^*, M^*)$ to the sample.}
    \item{Repeat until some iteration limit.}
\end{enumerate}

The simplest ABC algorithm is the ABC Rejection, and it goes very 
similarly to the generic algorithm we just presented, with the 
specification that on step~\ref{alg:abc_step1} the proposal distribution
is the prior distribution. The output of this algorithm is a sample of 
the distribution $p (\theta, M| d (\phi (M, \theta), D) \leq \epsilon)$; 
when $\epsilon \to \infty$ this is then a sample of the prior 
distribution, and as $\epsilon \to 0$ then this sample tends to be a
sample of the posterior distribution~\cite{Pritchard1999}. This 
algorithm, however, does not perform well when the posterior 
distribution is very different from the prior distribution. For that 
reason, new ABC methods using Monte Carlo Markov Chains were 
created~\cite{Marjoram2003}. The ABC MCMC method proposed by Marjoram et 
al. (2003) produces a Markov Chain whose stationary distribution is
$p (\theta, M | d (\phi (M, \theta), D) \leq \epsilon)$. Nonetheless, 
this algorithm might still suffer from correlation in samples or even
get stuck in regions of local peaks of probability. For that reason,
the ABC sequantial Monte Carlo (ABC SMC)method was 
proposed~\cite{Toni2009}.

%-> Explain ABC SMC
The ABC SMC method creates a sequence of samples with the goal of 
getting closer to a posterior sample in each step. Let we simplify the
notation by saying that $d(\phi (M, \theta), D)$ is just 
$\rho_{M, \theta}$. From a predefined sequence $\epsilon_1, \ldots, 
\epsilon_T$ the algorithm generates a sequence of samples that 
represents the distributions 
$p (\theta, M| \rho_{M, \theta} \leq \epsilon_1), 
p (\theta, M| \rho_{M, \theta} \leq \epsilon_2), \ldots,
p (\theta, M| \rho_{M, \theta} \leq \epsilon_T)$. At the first 
generation, a sample of 
$p (\theta, M| \rho_{M, \theta} \leq \epsilon_1)$ is created by 
proposing points from the prior distribution. Then, for next generations
the candidates to the sample are proposed based on the points of the 
last generation and their weight, plus some noise determined by a 
perturbation Kernel; the weight of a point $(\theta^*, M^*)$ on 
generation $t$ is a estimative of  
$p (\theta = \theta^*, M = M^* | \rho_{M, \theta} \leq \epsilon_t)$. The 
pseudo-code~\ref{code:abc_smc} presents the ABC SMC algorithm.

\begin{algorithm}[h]
\textsc{ABC SMC} $(\mathcal{M}, D)$
\begin{algorithmic}[1]
    \State Define the sequence $\epsilon_1, \ldots, \epsilon_T$.
    \State Define $N$, the sample size for each generation. 
    \State Sample $\{(\theta^{(1, 1)}, M^{(1, 1)}), 
                     (\theta^{(1, 2)}, M^{(1, 2)}), \ldots, 
                     (\theta^{(1, N)}, M^{(1, N)})\}$ from 
                     $p (\theta, M| D)$.
    \State Set $w^{(1, i)} = 1, \forall i \in {1, \ldots, N}$.  
    \For{$t \in \{1, \ldots, T\}$}
        \State $i \gets 1$
        \While{$i \leq N$}
            \State Sample $M^*$ from $p (M | D)$, the model prior.
            \State Sample $(\theta^{(t - 1, k)}, M^*)$ from the 
                last generation with weight $w^{(t - 1, k)}$.
            \State Create $(\theta^*, M^*)$ by perturbing 
                $\theta^{(t - 1, k)}$; 
                $\theta^* \propto K^t(\theta | \theta^{(t - 1, k)})$.
            \If{$p (\theta^* | M^*) = 0$}
                \State Continue to next iteration.
            \EndIf
            \State $D* \gets \phi (M^*, \theta^*)$
            \If{$d (D^*, D) \leq \epsilon$}
                \State $i \gets i + 1$
                \State $(\theta^{(t, i)}, M^{(t, i)}) \gets 
                    (\theta^*, M^*)$
            \EndIf
        \EndWhile
        \State Calculate the weights of the population:
            %\begin{equation*}
$                w^{(t, i)} = \frac{p (\theta^{(t, i)} | M^{(t, i)})}
                         {\sum_{j = 1}^N w^{(t-1, j)}p_{K^t}
                            (\theta^{(t-1, j)}, \theta^{(t, i)})}$
            %\end{equation*}
    \EndFor
    \Return
\end{algorithmic}
\caption{Pseudo-code of ABC SMC.}
\label{code:abc_smc}
\end{algorithm}
The ABC SMC algorithm is implemented in Python language in a software called 
ABC-SysBio~\cite{Liepe2014}.

